{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Linear Regression by Inventing Gradient Descent\n",
    "\n",
    "**Philosophy:** You won't be taught — you will *discover*. Every step builds on the last. Trust the process.\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll need\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "Run the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: Finding the Bottom of a Valley\n",
    "\n",
    "## Exercise 1.1 — Look Before You Leap\n",
    "\n",
    "Consider this function:\n",
    "\n",
    "$$f(x) = x^4 - 4x + 10$$\n",
    "\n",
    "Your first job is simply to **see** it.\n",
    "\n",
    "Use the plotting helper below to draw the function over the range $x \\in [-3, 3]$.\n",
    "\n",
    "```python\n",
    "# PLOTTING HELPER — run this cell as-is, you'll use plot_function() throughout the notebook\n",
    "def plot_function(f, x_range=(-3, 3), num_points=500, title=\"f(x)\", mark_x=None):\n",
    "    \"\"\"\n",
    "    Plots a function f over x_range.\n",
    "    Optionally marks a specific x value with a red dot.\n",
    "\n",
    "    Parameters:\n",
    "        f        : a Python function that takes a number and returns a number\n",
    "        x_range  : tuple (x_min, x_max)\n",
    "        title    : string label for the plot\n",
    "        mark_x   : if provided, draws a red dot at (mark_x, f(mark_x))\n",
    "    \"\"\"\n",
    "    xs = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    ys = [f(x) for x in xs]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(xs, ys, 'b-', linewidth=2)\n",
    "    if mark_x is not None:\n",
    "        plt.plot(mark_x, f(mark_x), 'ro', markersize=10, label=f'x={mark_x:.4f}, f(x)={f(mark_x):.4f}')\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Run the plotting helper cell to define `plot_function`.\n",
    "2. Define $f(x) = x^4 - 4x + 10$ as a Python function.\n",
    "3. Call `plot_function(f, x_range=(-3, 3), title=\"x^4 - 4x + 10\")`.\n",
    "4. Visually estimate: **where does the minimum appear to be?** Write your guess as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HELPER — run this cell as-is\n",
    "def plot_function(f, x_range=(-3, 3), num_points=500, title=\"f(x)\", mark_x=None):\n",
    "    xs = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    ys = [f(x) for x in xs]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(xs, ys, 'b-', linewidth=2)\n",
    "    if mark_x is not None:\n",
    "        plt.plot(mark_x, f(mark_x), 'ro', markersize=10, label=f'x={mark_x:.4f}, f(x)={f(mark_x):.4f}')\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Step 1: Define f(x) = x^4 - 4x + 10\n",
    "def f(x):\n",
    "    pass  # replace this\n",
    "\n",
    "# Step 2: Plot it\n",
    "# plot_function(...)\n",
    "\n",
    "# Step 3: Write your visual estimate of the minimum below\n",
    "# My guess: x ≈ ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 — Hit and Trial\n",
    "\n",
    "Now let's find the minimum by hand — no calculus, just trying values.\n",
    "\n",
    "The plotting helper accepts a `mark_x` argument. Use it to mark your guesses on the plot.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Start with your visual estimate from Exercise 1.1. Call `plot_function(f, mark_x=YOUR_GUESS)`.\n",
    "2. Look at the plot. Is the marked point at the bottom? If not, which direction should you move?\n",
    "3. Try a new value. Re-plot.\n",
    "4. Repeat until you are confident you have found the minimum to **2 decimal places**.\n",
    "5. Record **every guess you made and the function value at that guess** in a table (use a Python list of tuples).\n",
    "\n",
    "```python\n",
    "# Template for recording your guesses:\n",
    "guesses = [\n",
    "    # (x_value, f(x_value))\n",
    "    # e.g. (1.0, f(1.0)),\n",
    "]\n",
    "```\n",
    "\n",
    "**Reflection questions (answer in a markdown cell below):**\n",
    "- How many guesses did it take?\n",
    "- What strategy did you use to decide the next guess?\n",
    "- Can you describe your strategy in plain English, like a recipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — try different values of mark_x\n",
    "plot_function(f, mark_x=1.0, title=\"My guess: x=1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record all your guesses here\n",
    "guesses = [\n",
    "    # (x_value, f(x_value))\n",
    "]\n",
    "\n",
    "# Print them nicely\n",
    "print(f\"{'Guess #':<10} {'x':<15} {'f(x)':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for i, (x, fx) in enumerate(guesses):\n",
    "    print(f\"{i+1:<10} {x:<15.6f} {fx:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your reflection here:**\n",
    "\n",
    "- How many guesses did it take?\n",
    "- What was your strategy?\n",
    "- (Write in plain English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: Making the Machine Do the Searching\n",
    "\n",
    "## Exercise 2.1 — The Slope Tells You Which Way to Walk\n",
    "\n",
    "You had a strategy when doing hit-and-trial. Let's make it precise.\n",
    "\n",
    "Think about this: when you are standing on the curve at some point $x$, the **slope** of the curve tells you:\n",
    "- If slope is **positive** → the function is going *up* to the right → you should move *left* (decrease $x$)\n",
    "- If slope is **negative** → the function is going *up* to the left → you should move *right* (increase $x$)\n",
    "- If slope is **zero** → you are at the bottom!\n",
    "\n",
    "We can approximate the slope (derivative) at any point using:\n",
    "\n",
    "$$\\text{slope at } x \\approx \\frac{f(x + h) - f(x - h)}{2h} \\quad \\text{for a very small } h$$\n",
    "\n",
    "This is called the **numerical derivative**.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Write a function `numerical_derivative(f, x, h=1e-5)` that computes the slope at point `x`.\n",
    "2. Test it on $f(x) = x^4 - 4x + 10$ at a few points: $x = 0, 1, 1.5, 2$.\n",
    "3. Print both the slope value and whether it's positive, negative, or near-zero.\n",
    "4. Manually verify one of your answers: the true derivative is $f'(x) = 4x^3 - 4$. Does your numerical result match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    Returns the approximate derivative of f at point x.\n",
    "    \"\"\"\n",
    "    pass  # replace this\n",
    "\n",
    "\n",
    "# Test it\n",
    "test_points = [0, 1, 1.5, 2]\n",
    "for x in test_points:\n",
    "    slope = numerical_derivative(f, x)\n",
    "    # print x, slope, and direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 — One Step at a Time\n",
    "\n",
    "Now let's build the update rule.\n",
    "\n",
    "If you are at position $x$, and the slope there is $s$, your next position should be:\n",
    "\n",
    "$$x_{\\text{new}} = x - \\alpha \\cdot s$$\n",
    "\n",
    "where $\\alpha$ (alpha) is called the **learning rate** — it controls how big a step you take.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Start at $x_0 = 2.0$.\n",
    "2. Compute the slope at $x_0$.\n",
    "3. Compute $x_1 = x_0 - \\alpha \\cdot \\text{slope}$ with $\\alpha = 0.01$.\n",
    "4. Print $x_0$, slope, and $x_1$.\n",
    "5. Is $f(x_1) < f(x_0)$? It should be! Why?\n",
    "6. Now do this **manually** for 5 steps. Print each step. Is $x$ getting closer to the minimum you found in Exercise 1.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "alpha = 0.01\n",
    "x = 2.0\n",
    "\n",
    "print(f\"{'Step':<8} {'x':<15} {'f(x)':<15} {'slope':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for step in range(5):\n",
    "    slope = numerical_derivative(f, x)\n",
    "    print(f\"{step:<8} {x:<15.6f} {f(x):<15.6f} {slope:<15.6f}\")\n",
    "    x = x - alpha * slope  # the update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3 — Build `find_minima_1d`\n",
    "\n",
    "Now automate the process. Write a function that keeps taking steps until the slope is close enough to zero.\n",
    "\n",
    "**Stopping condition:** Stop when $|\\text{slope}| < \\epsilon$ (a small tolerance, e.g. $10^{-6}$), or when you've taken `max_steps` steps.\n",
    "\n",
    "```python\n",
    "def find_minima_1d(f, x_start, alpha=0.01, epsilon=1e-6, max_steps=10000):\n",
    "    \"\"\"\n",
    "    Finds the x that minimizes f using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        f         : function to minimize\n",
    "        x_start   : starting point\n",
    "        alpha     : learning rate\n",
    "        epsilon   : stop when |slope| < epsilon\n",
    "        max_steps : maximum number of steps\n",
    "\n",
    "    Returns:\n",
    "        x_min     : the x value at the minimum\n",
    "        f_min     : the function value at the minimum\n",
    "        history   : list of (step, x, f(x)) tuples\n",
    "    \"\"\"\n",
    "    pass  # YOUR IMPLEMENTATION\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Implement `find_minima_1d`.\n",
    "2. Test it on $f(x) = x^4 - 4x + 10$ starting from $x = 2.0$.\n",
    "3. Print the final answer and how many steps it took.\n",
    "4. Does it match your manual answer from Exercise 1.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def find_minima_1d(f, x_start, alpha=0.01, epsilon=1e-6, max_steps=10000):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test it\n",
    "x_min, f_min, history = find_minima_1d(f, x_start=2.0)\n",
    "print(f\"Minimum found at x = {x_min:.6f}\")\n",
    "print(f\"f(x_min) = {f_min:.6f}\")\n",
    "print(f\"Steps taken: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4 — Watch It Learn\n",
    "\n",
    "Use the plotting helper below to visualize the descent path. Run it after you have `history` from Exercise 2.3.\n",
    "\n",
    "```python\n",
    "# PLOTTING HELPER — descent path\n",
    "def plot_descent(f, history, x_range=(-3, 3), title=\"Gradient Descent\"):\n",
    "    \"\"\"\n",
    "    Plots the function and shows how x evolved during descent.\n",
    "    history: list of (step, x, f(x)) tuples\n",
    "    \"\"\"\n",
    "    xs = np.linspace(x_range[0], x_range[1], 500)\n",
    "    ys = [f(x) for x in xs]\n",
    "\n",
    "    steps, xvals, fvals = zip(*history)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Left: function with descent path\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(xs, ys, 'b-', linewidth=2, label='f(x)')\n",
    "    plt.plot(xvals, fvals, 'ro-', markersize=3, alpha=0.5, label='descent path')\n",
    "    plt.plot(xvals[-1], fvals[-1], 'g*', markersize=15, label=f'minimum: x={xvals[-1]:.4f}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x'); plt.ylabel('f(x)'); plt.legend(); plt.grid(True)\n",
    "\n",
    "    # Right: f(x) over steps\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, fvals, 'b-')\n",
    "    plt.title('f(x) value over steps')\n",
    "    plt.xlabel('step'); plt.ylabel('f(x)'); plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Define and run `plot_descent` with your history.\n",
    "2. What do you observe in the right plot? Is it always decreasing?\n",
    "3. Try starting from $x = -2.0$ instead. Does it converge to the same minimum? Why or why not?\n",
    "4. Try `alpha = 0.1`. What happens? Try `alpha = 1.0`. What happens? **Record your observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HELPER — run this cell as-is\n",
    "def plot_descent(f, history, x_range=(-3, 3), title=\"Gradient Descent\"):\n",
    "    xs = np.linspace(x_range[0], x_range[1], 500)\n",
    "    ys = [f(x) for x in xs]\n",
    "    steps, xvals, fvals = zip(*history)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(xs, ys, 'b-', linewidth=2, label='f(x)')\n",
    "    plt.plot(xvals, fvals, 'ro-', markersize=3, alpha=0.5, label='descent path')\n",
    "    plt.plot(xvals[-1], fvals[-1], 'g*', markersize=15, label=f'minimum: x={xvals[-1]:.4f}')\n",
    "    plt.title(title); plt.xlabel('x'); plt.ylabel('f(x)'); plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, fvals, 'b-')\n",
    "    plt.title('f(x) over steps'); plt.xlabel('step'); plt.ylabel('f(x)'); plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Experiment 1: Default settings\n",
    "x_min, f_min, history = find_minima_1d(f, x_start=2.0)\n",
    "plot_descent(f, history)\n",
    "\n",
    "# Experiment 2: Different starting point\n",
    "# ...\n",
    "\n",
    "# Experiment 3: Different learning rates\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations (fill in):**\n",
    "\n",
    "- Starting from x = -2.0: ...\n",
    "- With alpha = 0.1: ...\n",
    "- With alpha = 1.0: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: Stress-Testing Your Algorithm\n",
    "\n",
    "## Exercise 3.1 — New Functions, Same Algorithm\n",
    "\n",
    "Your `find_minima_1d` is general — it should work on any function. Let's test it.\n",
    "\n",
    "**For each function below:**\n",
    "1. Define it in Python.\n",
    "2. Plot it to visually estimate the minimum.\n",
    "3. Run `find_minima_1d` and record the result.\n",
    "4. Plot the descent path.\n",
    "5. Note any problems or surprises.\n",
    "\n",
    "**Functions to test:**\n",
    "\n",
    "| # | Function | x_range to plot | Suggested x_start |\n",
    "|---|----------|-----------------|-------------------|\n",
    "| A | $f(x) = x^6 - 4x^2 + 10$ | $(-2, 2)$ | $1.5$ |\n",
    "| B | $f(x) = (x - 3)^2 + 5$ | $(0, 6)$ | $0.0$ |\n",
    "| C | $f(x) = x^2 + 3\\|x\\|$ | $(-4, 4)$ | $2.0$ |\n",
    "| D | $f(x) = e^x - 5x$ | $(0, 4)$ | $3.0$ |\n",
    "| E | $f(x) = x^4 - 3x^3 + 2$ | $(-1, 3)$ | $2.5$ |\n",
    "\n",
    "> **Caution on Function A:** Plot it first. Does it have one minimum or more? What does that mean for your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function A\n",
    "def fA(x):\n",
    "    return x**6 - 4*x**2 + 10\n",
    "\n",
    "plot_function(fA, x_range=(-2, 2), title=\"fA: x^6 - 4x^2 + 10\")\n",
    "\n",
    "# YOUR CODE: run find_minima_1d and plot_descent\n",
    "# What do you notice? Are there multiple minima?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function B — YOUR CODE\n",
    "def fB(x):\n",
    "    pass\n",
    "\n",
    "# plot, find_minima_1d, plot_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function C — YOUR CODE\n",
    "def fC(x):\n",
    "    pass\n",
    "\n",
    "# plot, find_minima_1d, plot_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function D — YOUR CODE\n",
    "def fD(x):\n",
    "    pass\n",
    "\n",
    "# plot, find_minima_1d, plot_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function E — YOUR CODE\n",
    "def fE(x):\n",
    "    pass\n",
    "\n",
    "# plot, find_minima_1d, plot_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 — Function A Is Sneaky\n",
    "\n",
    "Plot Function A carefully. You should notice it has **two local minima** — one on the left and one on the right.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Run `find_minima_1d` on Function A starting from $x = 1.5$. Where does it end up?\n",
    "2. Run it starting from $x = -1.5$. Where does it end up?\n",
    "3. Run it starting from $x = 0.0$. Where does it end up?\n",
    "4. Which of the two minima is the **global** minimum (lowest overall)? Which is just a **local** minimum?\n",
    "5. Can your algorithm always find the global minimum? What would you need to do to be more confident?\n",
    "\n",
    "Write your answers in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "for x_start in [1.5, -1.5, 0.0]:\n",
    "    x_min, f_min, history = find_minima_1d(fA, x_start=x_start, alpha=0.01)\n",
    "    print(f\"Start: x={x_start:.1f}  →  Minimum at x={x_min:.6f}, f(x)={f_min:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your analysis:**\n",
    "\n",
    "- Global minimum: ...\n",
    "- Local minimum: ...\n",
    "- Can the algorithm always find the global minimum? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: Two Variables — A Landscape Instead of a Curve\n",
    "\n",
    "## Exercise 4.1 — See the Landscape\n",
    "\n",
    "Now consider a function of **two variables**:\n",
    "\n",
    "$$g(x, y) = x^2 + y^2$$\n",
    "\n",
    "This is a bowl shape in 3D. The minimum is clearly at $(x, y) = (0, 0)$.\n",
    "\n",
    "Use the 2D plotting helper below to visualize it.\n",
    "\n",
    "```python\n",
    "# PLOTTING HELPER — 2D contour plot\n",
    "def plot_function_2d(f2, x_range=(-3, 3), y_range=(-3, 3), num_points=100, title=\"f(x,y)\", mark_xy=None):\n",
    "    \"\"\"\n",
    "    Plots a 2-variable function as a contour map.\n",
    "\n",
    "    Parameters:\n",
    "        f2       : function that takes (x, y) and returns a scalar\n",
    "        x_range  : (x_min, x_max)\n",
    "        y_range  : (y_min, y_max)\n",
    "        mark_xy  : if provided as (x, y), draws a red dot at that point\n",
    "    \"\"\"\n",
    "    xs = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    ys = np.linspace(y_range[0], y_range[1], num_points)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = np.array([[f2(x, y) for x in xs] for y in ys])\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    contour = plt.contourf(X, Y, Z, levels=30, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    if mark_xy is not None:\n",
    "        plt.plot(mark_xy[0], mark_xy[1], 'r*', markersize=15,\n",
    "                 label=f'({mark_xy[0]:.3f}, {mark_xy[1]:.3f})')\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Define and run the `plot_function_2d` helper.\n",
    "2. Define $g(x, y) = x^2 + y^2$ and plot it.\n",
    "3. What color represents the minimum? What color represents high values?\n",
    "4. Now plot these functions and describe the shape of each landscape:\n",
    "   - $h_1(x, y) = (x - 1)^2 + (y + 2)^2$\n",
    "   - $h_2(x, y) = x^2 + 4y^2$\n",
    "   - $h_3(x, y) = x^2 + y^2 + x*y$\n",
    "5. Visually identify the minimum of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HELPER — run this cell as-is\n",
    "def plot_function_2d(f2, x_range=(-3, 3), y_range=(-3, 3), num_points=100, title=\"f(x,y)\", mark_xy=None):\n",
    "    xs = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    ys = np.linspace(y_range[0], y_range[1], num_points)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = np.array([[f2(x, y) for x in xs] for y in ys])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    contour = plt.contourf(X, Y, Z, levels=30, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    if mark_xy is not None:\n",
    "        plt.plot(mark_xy[0], mark_xy[1], 'r*', markersize=15,\n",
    "                 label=f'({mark_xy[0]:.3f}, {mark_xy[1]:.3f})')\n",
    "        plt.legend()\n",
    "    plt.title(title); plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def g(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "plot_function_2d(g, title=\"g(x,y) = x^2 + y^2\")\n",
    "\n",
    "# Now define and plot h1, h2, h3\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2 — Partial Derivatives\n",
    "\n",
    "With two variables, the slope has **two components** — one for each variable.\n",
    "\n",
    "The **partial derivative** with respect to $x$ tells you the slope if you move only in the $x$ direction (holding $y$ fixed).\n",
    "\n",
    "We can compute them numerically the same way:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x+h, y) - f(x-h, y)}{2h}$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial y} \\approx \\frac{f(x, y+h) - f(x, y-h)}{2h}$$\n",
    "\n",
    "Together, $(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})$ is called the **gradient** — it points in the direction of steepest *increase*.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Write a function `partial_x(f2, x, y, h=1e-5)` that returns $\\partial f / \\partial x$.\n",
    "2. Write a function `partial_y(f2, x, y, h=1e-5)` that returns $\\partial f / \\partial y$.\n",
    "3. Write a function `gradient_2d(f2, x, y, h=1e-5)` that returns the tuple `(df_dx, df_dy)`.\n",
    "4. Test on $g(x, y) = x^2 + y^2$ at several points:\n",
    "   - $(1, 1)$ → expected gradient: $(2, 2)$\n",
    "   - $(3, 0)$ → expected gradient: $(6, 0)$\n",
    "   - $(0, 0)$ → expected gradient: $(0, 0)$\n",
    "5. For $h_2(x, y) = x^2 + 4y^2$, compute the gradient at $(1, 1)$ and $(0, 2)$. Do the answers make intuitive sense? (Which direction does the gradient point — toward or away from the minimum?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def partial_x(f2, x, y, h=1e-5):\n",
    "    pass\n",
    "\n",
    "def partial_y(f2, x, y, h=1e-5):\n",
    "    pass\n",
    "\n",
    "def gradient_2d(f2, x, y, h=1e-5):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test on g(x,y) = x^2 + y^2\n",
    "test_points = [(1, 1), (3, 0), (0, 0)]\n",
    "for (x, y) in test_points:\n",
    "    grad = gradient_2d(g, x, y)\n",
    "    print(f\"Gradient at ({x}, {y}) = {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3 — Build `find_minima_2d`\n",
    "\n",
    "Now extend the 1D algorithm to 2D. The update rule becomes:\n",
    "\n",
    "$$x_{\\text{new}} = x - \\alpha \\cdot \\frac{\\partial f}{\\partial x}$$\n",
    "$$y_{\\text{new}} = y - \\alpha \\cdot \\frac{\\partial f}{\\partial y}$$\n",
    "\n",
    "Both variables are updated simultaneously.\n",
    "\n",
    "**Stopping condition:** Stop when $\\sqrt{(\\partial f/\\partial x)^2 + (\\partial f/\\partial y)^2} < \\epsilon$. This is the **magnitude** of the gradient (how steep the hill is overall).\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Implement `find_minima_2d(f2, x_start, y_start, alpha=0.1, epsilon=1e-6, max_steps=10000)`.\n",
    "   - It should return `(x_min, y_min, f_min, history)` where `history` is a list of `(step, x, y, f(x,y))` tuples.\n",
    "2. Test it on $g(x, y) = x^2 + y^2$ starting from $(2, 3)$. It should find $(0, 0)$.\n",
    "3. Test it on $h_1(x, y) = (x-1)^2 + (y+2)^2$ starting from $(3, 3)$. Expected minimum: $(1, -2)$.\n",
    "4. Test it on $h_2(x, y) = x^2 + 4y^2$ starting from $(2, 2)$. Expected minimum: $(0, 0)$.\n",
    "\n",
    "Use `plot_function_2d` with `mark_xy=(x_min, y_min)` to verify your answers visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def find_minima_2d(f2, x_start, y_start, alpha=0.1, epsilon=1e-6, max_steps=10000):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test on g\n",
    "x_min, y_min, f_min, history = find_minima_2d(g, x_start=2.0, y_start=3.0)\n",
    "print(f\"g: minimum at ({x_min:.4f}, {y_min:.4f}), f={f_min:.6f}, steps={len(history)}\")\n",
    "plot_function_2d(g, mark_xy=(x_min, y_min), title=\"g: descent result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on h1\n",
    "def h1(x, y):\n",
    "    return (x - 1)**2 + (y + 2)**2\n",
    "\n",
    "# YOUR CODE: run find_minima_2d and verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on h2\n",
    "def h2(x, y):\n",
    "    return x**2 + 4*y**2\n",
    "\n",
    "# YOUR CODE: run find_minima_2d and verify\n",
    "# Try alpha=0.1. Does it converge? Try alpha=0.3. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4 — Visualize the Path (2D)\n",
    "\n",
    "Use the helper below to draw the descent path on the contour map.\n",
    "\n",
    "```python\n",
    "# PLOTTING HELPER — 2D descent path\n",
    "def plot_descent_2d(f2, history, x_range=(-4, 4), y_range=(-4, 4), title=\"2D Gradient Descent\"):\n",
    "    \"\"\"\n",
    "    history: list of (step, x, y, f(x,y)) tuples\n",
    "    \"\"\"\n",
    "    xs = np.linspace(x_range[0], x_range[1], 100)\n",
    "    ys = np.linspace(y_range[0], y_range[1], 100)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = np.array([[f2(x, y) for x in xs] for y in ys])\n",
    "\n",
    "    steps, xvals, yvals, fvals = zip(*history)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(X, Y, Z, levels=30, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.plot(xvals, yvals, 'w-o', markersize=3, alpha=0.6, label='path')\n",
    "    plt.plot(xvals[0], yvals[0], 'rs', markersize=10, label='start')\n",
    "    plt.plot(xvals[-1], yvals[-1], 'g*', markersize=15, label='end')\n",
    "    plt.legend(); plt.title(title); plt.xlabel('x'); plt.ylabel('y')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, fvals, 'b-')\n",
    "    plt.title('f(x,y) over steps'); plt.xlabel('step'); plt.ylabel('f(x,y)'); plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Define and run `plot_descent_2d` for at least two of your test functions.\n",
    "2. Does the path head straight to the minimum, or does it zigzag? Why?\n",
    "3. For $h_2(x, y) = x^2 + 4y^2$: try `alpha=0.3` and `alpha=0.5`. Describe what you see. **This is a famous problem in gradient descent called oscillation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HELPER — run this cell as-is\n",
    "def plot_descent_2d(f2, history, x_range=(-4, 4), y_range=(-4, 4), title=\"2D Gradient Descent\"):\n",
    "    xs = np.linspace(x_range[0], x_range[1], 100)\n",
    "    ys = np.linspace(y_range[0], y_range[1], 100)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = np.array([[f2(x, y) for x in xs] for y in ys])\n",
    "    steps, xvals, yvals, fvals = zip(*history)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(X, Y, Z, levels=30, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.plot(xvals, yvals, 'w-o', markersize=3, alpha=0.6, label='path')\n",
    "    plt.plot(xvals[0], yvals[0], 'rs', markersize=10, label='start')\n",
    "    plt.plot(xvals[-1], yvals[-1], 'g*', markersize=15, label='end')\n",
    "    plt.legend(); plt.title(title); plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, fvals, 'b-')\n",
    "    plt.title('f(x,y) over steps'); plt.xlabel('step'); plt.ylabel('f(x,y)'); plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Run descent on h2 with different alphas and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 5: Going Generic\n",
    "\n",
    "## Exercise 5.1 — N Variables\n",
    "\n",
    "Your 1D and 2D algorithms are very similar. Can you unify them?\n",
    "\n",
    "Think about it this way:\n",
    "- In 1D, your position is a single number $x$. The gradient is a single number.\n",
    "- In 2D, your position is a pair $(x, y)$. The gradient is a pair $(\\partial f/\\partial x, \\partial f/\\partial y)$.\n",
    "- In N dimensions, your position is a **list** of N numbers. The gradient is also a **list** of N numbers.\n",
    "\n",
    "The update rule is always the same:\n",
    "\n",
    "$$\\text{variables}_{\\text{new}} = \\text{variables} - \\alpha \\cdot \\text{gradient}$$\n",
    "\n",
    "where both are lists.\n",
    "\n",
    "**The signature you are building:**\n",
    "\n",
    "```python\n",
    "def find_minima(f, variables, alpha=0.01, epsilon=1e-6, max_steps=100000):\n",
    "    \"\"\"\n",
    "    Finds the values of 'variables' that minimize f.\n",
    "\n",
    "    Parameters:\n",
    "        f         : a function that takes a LIST of values and returns a scalar\n",
    "                    e.g., if variables = [x, y], then f([x, y]) should work\n",
    "        variables : initial guess, a list of numbers [v1, v2, ..., vN]\n",
    "        alpha     : learning rate\n",
    "        epsilon   : stop when gradient magnitude < epsilon\n",
    "        max_steps : maximum number of steps\n",
    "\n",
    "    Returns:\n",
    "        min_vars  : list of variable values at the minimum\n",
    "        f_min     : function value at the minimum\n",
    "        history   : list of (step, variables_copy, f_value) tuples\n",
    "    \"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Key challenge:** Computing the gradient for a list of N variables.\n",
    "\n",
    "For variable $i$, the partial derivative is:\n",
    "$$\\frac{\\partial f}{\\partial v_i} \\approx \\frac{f(v_1, ..., v_i + h, ..., v_N) - f(v_1, ..., v_i - h, ..., v_N)}{2h}$$\n",
    "\n",
    "You need to perturb one variable at a time.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. First, write a helper `compute_gradient(f, variables, h=1e-5)` that returns the gradient as a list.\n",
    "2. Then write `find_minima(f, variables, ...)`.\n",
    "3. Test by re-running your earlier examples, but now with the new interface:\n",
    "   - 1D: `find_minima(lambda v: v[0]**4 - 4*v[0] + 10, [2.0])`\n",
    "   - 2D: `find_minima(lambda v: v[0]**2 + v[1]**2, [2.0, 3.0])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def compute_gradient(f, variables, h=1e-5):\n",
    "    \"\"\"\n",
    "    Computes numerical gradient of f at 'variables'.\n",
    "    variables: list of N numbers\n",
    "    Returns: list of N partial derivatives\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def find_minima(f, variables, alpha=0.01, epsilon=1e-6, max_steps=100000):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test 1D\n",
    "min_vars, f_min, history = find_minima(lambda v: v[0]**4 - 4*v[0] + 10, [2.0])\n",
    "print(f\"1D minimum: x = {min_vars[0]:.6f}, f = {f_min:.6f}\")\n",
    "\n",
    "# Test 2D\n",
    "min_vars, f_min, history = find_minima(lambda v: v[0]**2 + v[1]**2, [2.0, 3.0])\n",
    "print(f\"2D minimum: x = {min_vars[0]:.6f}, y = {min_vars[1]:.6f}, f = {f_min:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2 — Three Variables\n",
    "\n",
    "Test your generic `find_minima` on 3D functions:\n",
    "\n",
    "$$p(x, y, z) = (x - 1)^2 + (y - 2)^2 + (z + 3)^2$$\n",
    "\n",
    "The minimum is obviously at $(1, 2, -3)$.\n",
    "\n",
    "$$q(x, y, z) = x^2 + 2y^2 + 3z^2 - 4x + 6z$$\n",
    "\n",
    "For $q$: what is the minimum analytically? (Hint: complete the square for each variable.)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Define $p$ and $q$ as functions that take a list `[x, y, z]`.\n",
    "2. Run `find_minima` on both, starting from `[0, 0, 0]`.\n",
    "3. Verify your answer for $p$ is $(1, 2, -3)$.\n",
    "4. For $q$: first compute the answer analytically, then verify your algorithm found it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def p(v):\n",
    "    x, y, z = v\n",
    "    return (x - 1)**2 + (y - 2)**2 + (z + 3)**2\n",
    "\n",
    "def q(v):\n",
    "    x, y, z = v\n",
    "    return x**2 + 2*y**2 + 3*z**2 - 4*x + 6*z\n",
    "\n",
    "# Run find_minima on p\n",
    "min_vars, f_min, history = find_minima(p, [0.0, 0.0, 0.0])\n",
    "print(f\"p minimum: {min_vars}, f = {f_min:.6f}\")\n",
    "\n",
    "# Run find_minima on q\n",
    "# YOUR CODE\n",
    "\n",
    "# What is the analytical answer for q?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 6: The Real Problem — Fitting a Line to Data\n",
    "\n",
    "## Exercise 6.1 — What Does It Mean to Fit a Line?\n",
    "\n",
    "Suppose you have a dataset: some $x$ values and corresponding $y$ values. You want to find the **straight line** $\\hat{y} = mx + c$ that best fits the data.\n",
    "\n",
    "But what does \"best fit\" mean?\n",
    "\n",
    "For each data point $(x_i, y_i)$, your line predicts $\\hat{y}_i = m x_i + c$. The **error** for that point is $y_i - \\hat{y}_i$.\n",
    "\n",
    "**Mean Squared Error (MSE)** is defined as:\n",
    "\n",
    "$$\\text{MSE}(m, c) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (m x_i + c))^2$$\n",
    "\n",
    "A smaller MSE means the line fits the data better.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Here is some sample data — run the cell below.\n",
    "2. Plot the data points using the helper below.\n",
    "3. Try drawing lines with different $(m, c)$ on the same plot. Which looks like the best fit visually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA — run this cell as-is\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(0, 10, 30)\n",
    "y_data = 2.5 * X_data + 4.0 + np.random.randn(30) * 3  # true line: m=2.5, c=4.0 + noise\n",
    "\n",
    "print(f\"Number of data points: {len(X_data)}\")\n",
    "print(f\"X range: [{X_data.min():.1f}, {X_data.max():.1f}]\")\n",
    "print(f\"y range: [{y_data.min():.1f}, {y_data.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HELPER — data + line overlay\n",
    "def plot_data_and_line(X, y, m=None, c=None, title=\"Data\"):\n",
    "    \"\"\"\n",
    "    Plots data points. If m and c are given, also draws the line y = mx + c.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X, y, color='blue', label='data', zorder=5)\n",
    "    if m is not None and c is not None:\n",
    "        x_line = np.linspace(X.min(), X.max(), 200)\n",
    "        y_line = m * x_line + c\n",
    "        plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'y = {m:.2f}x + {c:.2f}')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X'); plt.ylabel('y')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Plot just the data\n",
    "plot_data_and_line(X_data, y_data, title=\"Raw Data\")\n",
    "\n",
    "# 2. Try some lines — which looks best?\n",
    "plot_data_and_line(X_data, y_data, m=1.0, c=5.0, title=\"m=1.0, c=5.0\")\n",
    "# try more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2 — Write the MSE Function\n",
    "\n",
    "Now write a function that quantifies how well a line fits the data.\n",
    "\n",
    "```python\n",
    "def mse(X, y, m, c):\n",
    "    \"\"\"\n",
    "    Computes Mean Squared Error for predicting y from X using line y = mx + c.\n",
    "\n",
    "    Parameters:\n",
    "        X : array of input values (shape: N,)\n",
    "        y : array of true output values (shape: N,)\n",
    "        m : slope of the line\n",
    "        c : intercept of the line\n",
    "\n",
    "    Returns:\n",
    "        mse_value : a single number\n",
    "    \"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Implement `mse`. Do NOT use any library function — write the formula yourself using a loop or numpy operations.\n",
    "2. Test it:\n",
    "   - A perfect line should have MSE = 0. Create a tiny dataset where you know the exact answer: `X=[1,2,3]`, `y=[3,5,7]` is perfectly fit by $m=2, c=1$. Verify `mse(X, y, 2, 1) == 0`.\n",
    "   - For our main dataset, compute MSE for:\n",
    "     - `m=2.5, c=4.0` (the true line used to generate data)\n",
    "     - `m=1.0, c=5.0`\n",
    "     - `m=5.0, c=0.0`\n",
    "3. Which $(m, c)$ gives the lowest MSE? Does this match your visual judgment from Exercise 6.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def mse(X, y, m, c):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "X_tiny = np.array([1.0, 2.0, 3.0])\n",
    "y_tiny = np.array([3.0, 5.0, 7.0])\n",
    "print(f\"MSE for perfect line (should be 0): {mse(X_tiny, y_tiny, 2.0, 1.0):.6f}\")\n",
    "\n",
    "# Compare on main dataset\n",
    "lines_to_test = [\n",
    "    (2.5, 4.0, \"true line\"),\n",
    "    (1.0, 5.0, \"guess 1\"),\n",
    "    (5.0, 0.0, \"guess 2\"),\n",
    "]\n",
    "for m, c, label in lines_to_test:\n",
    "    error = mse(X_data, y_data, m, c)\n",
    "    print(f\"{label:15s}: m={m}, c={c}  →  MSE={error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.3 — The MSE Landscape\n",
    "\n",
    "MSE is a function of $m$ and $c$. Let's visualize it as a 2D landscape.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Use `plot_function_2d` (from Part 4) to plot MSE over a grid of $(m, c)$ values.\n",
    "   - Let $m$ range from 0 to 5, and $c$ from -5 to 15.\n",
    "   - You'll need to wrap `mse` so it takes a form compatible with `plot_function_2d`.\n",
    "2. What shape does the MSE landscape have? Is it convex? Does it have a clear minimum?\n",
    "3. Mark the point $(m, c) = (2.5, 4.0)$ on the plot. Is it at or near the minimum?\n",
    "\n",
    "**Hint for wrapping:**\n",
    "```python\n",
    "def mse_landscape(m, c):\n",
    "    return mse(X_data, y_data, m, c)\n",
    "\n",
    "plot_function_2d(mse_landscape, x_range=(0, 5), y_range=(-5, 15), title=\"MSE Landscape (m, c)\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def mse_landscape(m, c):\n",
    "    return mse(X_data, y_data, m, c)\n",
    "\n",
    "plot_function_2d(mse_landscape, x_range=(0, 5), y_range=(-5, 15),\n",
    "                 title=\"MSE Landscape\",\n",
    "                 mark_xy=(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 7: Putting It All Together — Linear Regression via Gradient Descent\n",
    "\n",
    "## Exercise 7.1 — Minimize MSE with `find_minima`\n",
    "\n",
    "You now have:\n",
    "- `find_minima(f, variables)` — a general minimizer\n",
    "- `mse(X, y, m, c)` — the function to minimize\n",
    "\n",
    "Put them together.\n",
    "\n",
    "**The key step:** To use `find_minima`, you need to express MSE as a function of a **list** `[m, c]`:\n",
    "\n",
    "```python\n",
    "def mse_for_optimizer(params):\n",
    "    m, c = params\n",
    "    return mse(X_data, y_data, m, c)\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Define `mse_for_optimizer` as above.\n",
    "2. Call `find_minima(mse_for_optimizer, [0.0, 0.0])` with an appropriate learning rate.\n",
    "   - **Warning:** You may need to tune `alpha`. Try `0.001` first, then adjust.\n",
    "3. Print the final `m` and `c` found by the algorithm.\n",
    "4. Compare to the true values `m=2.5, c=4.0`. How close did you get?\n",
    "5. Plot the resulting line over the data using `plot_data_and_line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def mse_for_optimizer(params):\n",
    "    m, c = params\n",
    "    return mse(X_data, y_data, m, c)\n",
    "\n",
    "\n",
    "# Run the optimizer — tune alpha if needed\n",
    "min_params, f_min, history = find_minima(mse_for_optimizer, [0.0, 0.0], alpha=0.001)\n",
    "\n",
    "m_found, c_found = min_params\n",
    "print(f\"Found:  m = {m_found:.4f},  c = {c_found:.4f}\")\n",
    "print(f\"True:   m = 2.5000,  c = 4.0000\")\n",
    "print(f\"MSE at found values: {mse(X_data, y_data, m_found, c_found):.4f}\")\n",
    "\n",
    "# Plot the result\n",
    "plot_data_and_line(X_data, y_data, m=m_found, c=c_found, title=f\"Best fit: m={m_found:.2f}, c={c_found:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.2 — Watch the Line Learn\n",
    "\n",
    "Let's visualize how the line evolves during gradient descent.\n",
    "\n",
    "Use the plotting helper below:\n",
    "\n",
    "```python\n",
    "# PLOTTING HELPER — animated learning\n",
    "def plot_learning_progress(X, y, history, steps_to_show=[0, 5, 20, 100, 500, -1]):\n",
    "    \"\"\"\n",
    "    Shows the fitted line at different stages of training.\n",
    "    history: list of (step, [m, c], mse_value) tuples from find_minima\n",
    "    \"\"\"\n",
    "    n = len(steps_to_show)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4*n, 4))\n",
    "\n",
    "    history_len = len(history)\n",
    "    x_line = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "    for ax, step_idx in zip(axes, steps_to_show):\n",
    "        if step_idx == -1:\n",
    "            step_idx = history_len - 1\n",
    "        step_idx = min(step_idx, history_len - 1)\n",
    "\n",
    "        step_num, params, mse_val = history[step_idx]\n",
    "        m, c = params\n",
    "        y_line = m * x_line + c\n",
    "\n",
    "        ax.scatter(X, y, color='blue', s=15, alpha=0.6)\n",
    "        ax.plot(x_line, y_line, 'r-', linewidth=2)\n",
    "        ax.set_title(f'Step {step_num}\\nm={m:.2f}, c={c:.2f}\\nMSE={mse_val:.1f}')\n",
    "        ax.set_xlabel('X'); ax.set_ylabel('y')\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Run `find_minima` with `alpha=0.001` starting from `[0.0, 0.0]`.\n",
    "2. Call `plot_learning_progress` with your history.\n",
    "3. Describe what you see: how does the line change from step 0 to the final step?\n",
    "4. Also plot the MSE over steps using the 2D descent helper.\n",
    "\n",
    "> **Note:** For this to work, your `find_minima` history must store `[m, c]` as a list (not just the step index). Make sure your implementation stores a copy of the variables at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HELPER — run this cell as-is\n",
    "def plot_learning_progress(X, y, history, steps_to_show=[0, 5, 20, 100, 500, -1]):\n",
    "    n = len(steps_to_show)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4*n, 4))\n",
    "    history_len = len(history)\n",
    "    x_line = np.linspace(X.min(), X.max(), 100)\n",
    "    for ax, step_idx in zip(axes, steps_to_show):\n",
    "        if step_idx == -1:\n",
    "            step_idx = history_len - 1\n",
    "        step_idx = min(step_idx, history_len - 1)\n",
    "        step_num, params, mse_val = history[step_idx]\n",
    "        m, c = params\n",
    "        y_line = m * x_line + c\n",
    "        ax.scatter(X, y, color='blue', s=15, alpha=0.6)\n",
    "        ax.plot(x_line, y_line, 'r-', linewidth=2)\n",
    "        ax.set_title(f'Step {step_num}\\nm={m:.2f}, c={c:.2f}\\nMSE={mse_val:.1f}')\n",
    "        ax.set_xlabel('X'); ax.set_ylabel('y'); ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "min_params, f_min, history = find_minima(mse_for_optimizer, [0.0, 0.0], alpha=0.001)\n",
    "\n",
    "# Visualize the learning process\n",
    "plot_learning_progress(X_data, y_data, history, steps_to_show=[0, 5, 20, 100, 500, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3 — New Dataset, Same Code\n",
    "\n",
    "Here are three new datasets. For each:\n",
    "1. Plot the raw data.\n",
    "2. Use your `find_minima` + `mse` to find the best-fit line.\n",
    "3. Plot the fitted line over the data.\n",
    "4. Report the final $m$, $c$, and MSE.\n",
    "\n",
    "No new code is needed — you're reusing everything you built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETS — run this cell as-is\n",
    "np.random.seed(7)\n",
    "\n",
    "# Dataset 1: negative slope\n",
    "X1 = np.linspace(0, 10, 40)\n",
    "y1 = -1.5 * X1 + 20 + np.random.randn(40) * 2\n",
    "\n",
    "# Dataset 2: steep positive slope\n",
    "X2 = np.linspace(-5, 5, 50)\n",
    "y2 = 5.0 * X2 - 10 + np.random.randn(50) * 4\n",
    "\n",
    "# Dataset 3: nearly flat\n",
    "X3 = np.linspace(0, 20, 60)\n",
    "y3 = 0.3 * X3 + 2.0 + np.random.randn(60) * 5\n",
    "\n",
    "print(\"Datasets ready: X1/y1, X2/y2, X3/y3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Dataset 1\n",
    "plot_data_and_line(X1, y1, title=\"Dataset 1 — Raw\")\n",
    "\n",
    "def mse_d1(params):\n",
    "    m, c = params\n",
    "    return mse(X1, y1, m, c)\n",
    "\n",
    "# find_minima and plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Dataset 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Dataset 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.4 — Sanity Check with NumPy\n",
    "\n",
    "NumPy has a function `np.polyfit(X, y, 1)` that finds the best-fit line using an exact mathematical formula (not gradient descent).\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Run `np.polyfit(X_data, y_data, 1)` — it returns `[m, c]`.\n",
    "2. Compare the result to what your gradient descent found.\n",
    "3. Run the same comparison for all three datasets in Exercise 7.3.\n",
    "4. Are the answers the same? Should they be exactly the same, or just approximately the same? Why?\n",
    "\n",
    "**Bonus:** If your answers disagree significantly (more than 0.01), investigate why. Check:\n",
    "- Did gradient descent converge? (Check the final gradient magnitude.)\n",
    "- Is your learning rate too large or too small?\n",
    "- Did you run enough steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "m_np, c_np = np.polyfit(X_data, y_data, 1)\n",
    "print(f\"NumPy:          m = {m_np:.4f},  c = {c_np:.4f}\")\n",
    "\n",
    "# Compare with gradient descent result from Exercise 7.1\n",
    "print(f\"Gradient Desc:  m = {m_found:.4f},  c = {c_found:.4f}\")\n",
    "\n",
    "# Repeat for other datasets\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 8: Bonus Challenges\n",
    "\n",
    "If you've made it this far, you've independently discovered **gradient descent** and used it to implement **linear regression**. These are the foundations of almost every machine learning algorithm.\n",
    "\n",
    "Here are some open-ended challenges to push further:\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge A — What if the data isn't linear?\n",
    "\n",
    "Generate this dataset:\n",
    "\n",
    "```python\n",
    "X_quad = np.linspace(-3, 3, 50)\n",
    "y_quad = 1.5 * X_quad**2 - 2 * X_quad + 1 + np.random.randn(50) * 1.5\n",
    "```\n",
    "\n",
    "The data was generated with a **quadratic** function $y = 1.5x^2 - 2x + 1$.\n",
    "\n",
    "1. Try fitting a straight line to it. What MSE do you get?\n",
    "2. Now fit a quadratic: $\\hat{y} = ax^2 + bx + c$. You now have **3 parameters** to optimize: $[a, b, c]$.\n",
    "3. Extend your `mse` function to handle any polynomial and minimize it with `find_minima`.\n",
    "4. Compare the fitted $a, b, c$ to the true values $1.5, -2, 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge A — YOUR CODE\n",
    "X_quad = np.linspace(-3, 3, 50)\n",
    "y_quad = 1.5 * X_quad**2 - 2 * X_quad + 1 + np.random.randn(50) * 1.5\n",
    "\n",
    "# Step 1: fit a straight line, report MSE\n",
    "# Step 2: define mse_quadratic(params) where params = [a, b, c]\n",
    "# Step 3: find_minima(mse_quadratic, [0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge B — Learning Rate Scheduler\n",
    "\n",
    "You've seen that a large $\\alpha$ causes oscillation and a small $\\alpha$ is slow.\n",
    "\n",
    "One trick: **decay the learning rate** over time. Start with a large $\\alpha$ and make it smaller each step:\n",
    "\n",
    "$$\\alpha_t = \\frac{\\alpha_0}{1 + t \\cdot \\text{decay\\_rate}}$$\n",
    "\n",
    "Modify your `find_minima` to support an optional `decay_rate` parameter. Compare:\n",
    "- Fixed `alpha=0.1` on the MSE problem\n",
    "- Decaying alpha starting at `0.1` with `decay_rate=0.01`\n",
    "\n",
    "Which converges faster? Plot both loss curves on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge B — YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge C — Stochastic Gradient Descent\n",
    "\n",
    "In real ML, datasets have millions of points. Computing MSE over all of them every step is expensive.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):** Instead of using all data to compute the gradient, use a **random single point** (or a small batch of points) each step.\n",
    "\n",
    "1. Modify `find_minima` (or write `find_minima_sgd`) to:\n",
    "   - Each step, pick one random data point $(x_i, y_i)$\n",
    "   - Compute the gradient of the error for that single point\n",
    "   - Update $m$ and $c$\n",
    "2. Run it on `X_data, y_data`.\n",
    "3. Plot the loss curve. Is it smooth or noisy? Why?\n",
    "4. How does the final answer compare to full gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge C — YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection — What Did You Just Build?\n",
    "\n",
    "Take a moment to answer these questions in your own words:\n",
    "\n",
    "1. **What is gradient descent?** Explain it in 3 sentences as if you're describing it to someone who has never heard of it.\n",
    "\n",
    "2. **What is the learning rate?** What happens if it's too large? Too small?\n",
    "\n",
    "3. **What is MSE?** Why do we minimize it instead of just minimizing the average error?\n",
    "\n",
    "4. **What is linear regression?** What problem does it solve?\n",
    "\n",
    "5. **What is the connection** between minimizing MSE and fitting a line?\n",
    "\n",
    "6. **What's the biggest limitation** of the approach you built? What would you want to improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. Gradient descent is...\n",
    "\n",
    "2. The learning rate is...\n",
    "\n",
    "3. MSE is...\n",
    "\n",
    "4. Linear regression is...\n",
    "\n",
    "5. The connection is...\n",
    "\n",
    "6. The biggest limitation is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*You've just invented gradient descent and linear regression from scratch. That's not nothing — it's the foundation of deep learning.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
